{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da057046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./lib/customCallbacks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './lib/customCallbacks.py'\n",
    "# -------------------------------------------------------------------------------------------------------------------------------\n",
    "import tensorflow as tf\n",
    "# customised learning rate scheduler\n",
    "# considering slope of validation loss\n",
    "class lossGradientLR(tf.keras.callbacks.Callback):\n",
    "    \"\"\"gradually descend learning rate if the loss defined my the model is not descending at desired rate\"\"\"\n",
    "    def __init__(self,initial_learning_rate,patience=5,\n",
    "                 slope=0.25,factor=0.75,lr_least=1e-10,verbose=False):\n",
    "        \"\"\"arguments:\n",
    "        ----------\n",
    "        initial_learning_rate : the starting value of learning rate\n",
    "        patience : minimum number of epochs before manipulation\n",
    "        slope : loss gradient descent threshold\n",
    "        factor : multiplying factor for learning rate change\n",
    "        lr_least : least value of learning rate to apply\n",
    "        verbose : status message display control flag\n",
    "        \"\"\"\n",
    "        super(lossGradientLR, self).__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.patience = patience\n",
    "        self.slope = slope\n",
    "        self.factor = factor\n",
    "        self.lr_least = lr_least\n",
    "        self.verbose = verbose\n",
    "        self.loss = []\n",
    "        self.lr = [initial_learning_rate]\n",
    "        self.lastAdjust = 0\n",
    "        \n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        \"\"\"review learning rate & calculate\n",
    "        arguments:\n",
    "        ----------\n",
    "        epoch : number of the epoch that ended\n",
    "        logs : learning evaluation logs\n",
    "        \"\"\"\n",
    "        # fetch validation loss\n",
    "        self.loss.append(logs.get('val_loss'))\n",
    "        \n",
    "        from tensorflow.keras import backend as K\n",
    "        \n",
    "        if self.lr[-1]<=self.lr_least: # no updates if at lr_least\n",
    "            return        \n",
    "        \n",
    "        # checkpoints\n",
    "        flag1 = (epoch-self.lastAdjust)>self.patience # min epoch before adjust\n",
    "        try:\n",
    "            flag2 = (self.loss[-1]-self.loss[-2])>0 and (self.loss[-2]-self.loss[-3])>0\n",
    "            # if two continuous epochs have increasing validation loss\n",
    "        except:\n",
    "            flag2 = False # fails in first 4 epochs, handle it\n",
    "        \n",
    "        # slope of validation loss\n",
    "        lossGrad = self.loss[-self.patience:][0] - self.loss[-self.patience:][-1]\n",
    "        lossGrad /= self.loss[-self.patience:][0]\n",
    "        \n",
    "        # if slope of validation loss not descending as expected by slope\n",
    "        # or flag2 : if two continuous epochs have increasing validation loss\n",
    "        if (flag1 and lossGrad<self.slope) or (flag2):\n",
    "            self.lr.append(max(self.lr[-1]*self.factor,self.lr_least)) #min lr_least\n",
    "            self.lastAdjust = epoch # update last adjust epoch\n",
    "            \n",
    "        # update model learning rate\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.lr[-1])\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\n\\n-------------------------------------------------------\")\n",
    "            print(\"slopeOfLoss:%.2f\"%lossGrad,\" @ epoch:%3d\"%epoch, \"---- learning_rate:\",self.lr[-1])\n",
    "            print(\"-------------------------------------------------------\\n\")\n",
    "            \n",
    "        super().on_epoch_end(epoch, logs)\n",
    "        \n",
    "# report learning rate to tensorboard\n",
    "# due to custom lr_scheduler\n",
    "class LRTensorBoard(tf.keras.callbacks.TensorBoard):\n",
    "    \"\"\"report to tensorfboard about the updates created by lossGradientLR\"\"\"\n",
    "    def __init__(self, log_dir, **kwargs):\n",
    "        super(LRTensorBoard,self).__init__(log_dir=log_dir, **kwargs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        from tensorflow.keras import backend as K\n",
    "        logs = logs or {}\n",
    "        logs.update({'learning_rate': K.eval(self.model.optimizer.learning_rate)})\n",
    "        super().on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362e3ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./lib/customCallbacks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a './lib/customCallbacks.py'\n",
    "# -------------------------------------------------------------------------------------------------------------------------------\n",
    "class eStop(tf.keras.callbacks.Callback):\n",
    "    \"\"\"stop training if gradient descent is ineffective over few epochs\"\"\"\n",
    "    def __init__(self,patience=11,slope=0.25):\n",
    "        \"\"\"arguments:\n",
    "        ----------\n",
    "        patience : Number of epochs with no improvement after which training will be stopped\n",
    "        slope = loss gradient descent threshold\n",
    "        \"\"\"\n",
    "        super(eStop,self).__init__()\n",
    "        self.patience = patience\n",
    "        self.slope = slope\n",
    "        self.loss = []\n",
    "        \n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        \"\"\"review losses, decide EarlyStop implementaion\n",
    "        arguments:\n",
    "        ----------\n",
    "        epoch : number of the epoch that ended\n",
    "        logs : learning evaluation logs\n",
    "        \"\"\"\n",
    "        \n",
    "        # fetch validation loss\n",
    "        self.loss.append(logs.get('val_loss'))\n",
    "        \n",
    "        # allow minimum number of epoch\n",
    "        if len(self.loss)<self.patience:\n",
    "            return\n",
    "        \n",
    "        # compute slope of loss\n",
    "        lossGrad = self.loss[-self.patience:][0] - self.loss[-self.patience:][-1]\n",
    "        lossGrad /= self.loss[-self.patience:][0]\n",
    "        if lossGrad<self.slope: # if threshold attained\n",
    "            self.model.stop_training = True # stop training\n",
    "            print(\"\\n\\n---------------------------------------------------------------\")\n",
    "            print(\"eStop initiated after epoch %3d for poor loss gradient of %.3f\"%(epoch,lossGrad))\n",
    "            print(\"---------------------------------------------------------------\\n\")\n",
    "            return\n",
    "        \n",
    "        super().on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aaf2d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./lib/customCallbacks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a './lib/customCallbacks.py'\n",
    "# -------------------------------------------------------------------------------------------------------------------------------\n",
    "class layerUnFreeze(tf.keras.callbacks.Callback):\n",
    "    \"\"\"control layer trainable status over the epoch\"\"\"\n",
    "    def __init__(self,uncontrolled=-5,schedule={}):\n",
    "        \"\"\"arguments:\n",
    "        ----------\n",
    "        uncontrolled : number of layers (from the output end) to be left trainable always eg. -15 or -5 from output\n",
    "        schedule : dictionary of % of layers as values mapped to epochs as keys in ascending order\n",
    "        this defines the % of layers to be open for training from the respective epochs\n",
    "        this affects only the layes not marked by 'uncontrolled'\n",
    "        eg. {20:0.25, 25:0.5, 30:0.75, 35:0.9}\n",
    "        \"\"\"\n",
    "        super(layerUnFreeze,self).__init__()\n",
    "        self.uncontrolled = uncontrolled\n",
    "        self.schedule = schedule\n",
    "        \n",
    "    def on_train_begin(self,logs=None):\n",
    "        \"\"\"preset the layer trainable layers\"\"\"\n",
    "        self.total = len(self.model.layers)\n",
    "        for i,layer in enumerate(self.model.layers):\n",
    "            if i < self.total+self.uncontrolled:\n",
    "                layer.trainable = False # controlled layers\n",
    "            else:\n",
    "                layer.trainable = True # uncontrolled layers\n",
    "                \n",
    "        # schedule of layers to unfreeze\n",
    "        import numpy as np\n",
    "        epochs = np.arange(1,list(self.schedule.keys())[-1]+1)\n",
    "        self.trainables = np.round(np.array(list(map(self.schedule.get,\n",
    "                                                     epochs,[0]*len(epochs))))*(self.total+self.uncontrolled)).astype(int)        \n",
    "        for i in range(1,len(self.trainables)):\n",
    "            if self.trainables[i]==0:\n",
    "                self.trainables[i]=self.trainables[i-1]\n",
    "        self.trainables = -self.trainables+self.uncontrolled\n",
    "        \n",
    "        # invoke super method\n",
    "        super().on_train_begin(logs)\n",
    "        \n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        \"\"\"unfreeze layers as per schedule\"\"\"\n",
    "        if epoch>=len(self.trainables):\n",
    "            unfreeze = self.trainables[-1]\n",
    "        else:\n",
    "            unfreeze = self.trainables[epoch]\n",
    "        for layer in self.model.layers[unfreeze:]:\n",
    "            layer.trainable = True #unfreezed\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "        \n",
    "# report trainables to tensorboard\n",
    "class trainableReport(tf.keras.callbacks.TensorBoard):\n",
    "    \"\"\"report to tensorboard about the updates created by layerUnFreeze\"\"\"\n",
    "    def __init__(self, log_dir, **kwargs):\n",
    "        super(trainableReport,self).__init__(log_dir=log_dir, **kwargs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        trainable=0\n",
    "        for layer in self.model.layers:\n",
    "            trainable+= layer.trainable\n",
    "        logs.update({'Trainable': trainable})\n",
    "        super().on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f1b8cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./lib/customCallbacks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a './lib/customCallbacks.py'\n",
    "# -------------------------------------------------------------------------------------------------------------------------------\n",
    "from timeit import default_timer as timer\n",
    "class timeLog(tf.keras.callbacks.TensorBoard):\n",
    "    \"\"\"to log epoch times as a measure of computational resource consumption\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir, **kwargs):\n",
    "        super(timeLog,self).__init__(log_dir=log_dir, **kwargs)\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.tBegin = timer()        \n",
    "        super().on_epoch_begin(epoch, logs)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logs = logs or {}\n",
    "        logs.update({'lapTime':(timer()-self.tBegin)})\n",
    "        super().on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b5db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.customCallbacks import lossGradientLR,LRTensorBoard,eStop,layerUnFreeze,trainableReport,timeLog"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
