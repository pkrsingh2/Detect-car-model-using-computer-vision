{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1619256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./lib/networkPipe.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile './lib/networkPipe.py'\n",
    "# -------------------------------------------------------------------------------------------------------------------------------\n",
    "import tensorflow as tf\n",
    "# fetch GPU device name\n",
    "GPU = tf.config.list_logical_devices('GPU')[0].name\n",
    "\n",
    "# compute Intersection-over-Union metrics\n",
    "def IOU(y_true,y_pred):\n",
    "    \"\"\"arguments:\n",
    "    ----------\n",
    "    y_true : true bounding box coordinates as list\n",
    "    y_pred : predicted bounding box coordinates as list\n",
    "    expected shape of arguments: (None,4)\n",
    "    \n",
    "    returns: bounding box metric, Intersection-over-Union as list of shape (None,)\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import backend as K\n",
    "    \n",
    "    with tf.device(GPU):\n",
    "        # intersection coordinates (x1,y1)\n",
    "        x1 = tf.maximum(y_true[:,0],y_pred[:,0])\n",
    "        y1 = tf.maximum(y_true[:,1],y_pred[:,1])\n",
    "        # intersection coordinates (x2,y2)\n",
    "        x2 = tf.minimum(y_true[:,2],y_pred[:,2])\n",
    "        y2 = tf.minimum(y_true[:,3],y_pred[:,3])    \n",
    "        # compute areas\n",
    "        inter_area = (x2 - x1) * (y2 - y1)\n",
    "        union_area = (y_true[:,2] - y_true[:,0]) * (y_true[:,3] - y_true[:,1]) # true bBox\n",
    "        union_area += (y_pred[:,2] - y_pred[:,0]) * (y_pred[:,3] - y_pred[:,1]) # predicted bBox\n",
    "        union_area -= inter_area # intersection area\n",
    "        return (inter_area / (union_area + K.epsilon()))\n",
    "\n",
    "def IoU(y_true, y_pred):\n",
    "    \"\"\"wraps the IOU function into TensorFlow op for eager execution\"\"\"\n",
    "    return tf.py_function(IOU,[y_true, y_pred],tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a780d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./lib/networkPipe.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a './lib/networkPipe.py'\n",
    "# -------------------------------------------------------------------------------------------------------------------------------\n",
    "class Pipe():\n",
    "    \"\"\"pipeline for ETL-to-Learning-to-Evaluation-&-Reporting\"\"\"\n",
    "    \n",
    "    def __init__(self,generatorParams,modelFunc,modelName,modelParams={},weights=None):\n",
    "        \"\"\"arguments:\n",
    "        ----------\n",
    "        generatorParams : dict of kwargs to multiGen(), positional arguments excluded\n",
    "        modelFunc : method that will return an assembled model\n",
    "        modelName : representative name for the purpose of record & comparison\n",
    "        modelParams : dict of arguments to modelFunc, except input_size\n",
    "        weights : path of pretrained weights file\n",
    "        \"\"\"\n",
    "        \n",
    "        from lib.extract import trainDF,testDF\n",
    "        from lib.generator import multiGen\n",
    "        \n",
    "        import os\n",
    "        \n",
    "        # for future reference\n",
    "        self.trainDF,self.testDF = trainDF,testDF\n",
    "        \n",
    "        # generator implementation\n",
    "        gParams = dict(train=trainDF,test=testDF,imagePath=\"ImagePath\",\n",
    "                       className=\"className\",bBox=[\"x1\",\"y1\",\"x2\",\"y2\"],\n",
    "                       input_size=[\"width\",\"height\"],validation_split=0.1)\n",
    "        gParams.update(**generatorParams)\n",
    "        myGen = multiGen(**gParams)\n",
    "        self.trainset = myGen.subset('training')\n",
    "        self.validationset = myGen.subset('validation')\n",
    "        self.testset = myGen.subset('testing')\n",
    "        self.evalTrain = myGen.subset('evaluation on trainset')\n",
    "        self.evalTest = myGen.subset('evaluation on testset')\n",
    "        del myGen\n",
    "        \n",
    "        # batch output shape for reference\n",
    "        self.batch_size = generatorParams.get('batch_size')\n",
    "        self.target_size = generatorParams.get('target_size')\n",
    "        self.input_size = tuple((self.batch_size,self.target_size[1],self.target_size[0],3))\n",
    "\n",
    "        # update model parameters\n",
    "        self.mParams = dict(input_size=self.input_size)\n",
    "        self.mParams.update(**modelParams)\n",
    "        \n",
    "        # inititate model network\n",
    "        self.mFunc = modelFunc\n",
    "        self.model = self.mFunc(**self.mParams)\n",
    "        self.model.load_weights(weights) if weights!=None else None\n",
    "        \n",
    "        # store other arguments\n",
    "        self.modelName = modelName\n",
    "        \n",
    "        # create logs folder\n",
    "        try:\n",
    "            os.mkdir('logs')\n",
    "            os.mkdir('deployables')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # record & create model Log\n",
    "        self.mLog = dict(name=modelName)\n",
    "        self.mLog.update(**generatorParams)\n",
    "        \n",
    "        # fitting status\n",
    "        self._compiled=False\n",
    "        self._fitted=False\n",
    "        \n",
    "    def compiler(self,optim=tf.keras.optimizers.Adam(),lrScheduleParams={},eStopParams={},\n",
    "                 trainabilityParams={},gridPoint={},sBest=False):\n",
    "        \"\"\" compile the model & other callbacks\n",
    "        arguments:\n",
    "        ----------\n",
    "        optimParams : dict of arguments to RMSprop, except learning_rate\n",
    "        lrScheduleParams : dict of arguments to lossGradientLR scheduler, except initial_learning_rate\n",
    "        eStopParams : dict of arguments to eStop callback instance\n",
    "        trainabilityParams : dict of arguments to layerUnFreeze callback\n",
    "        gridPoint : dict of other hyperparameters\n",
    "        sBest : option to saveBest weights\n",
    "        \"\"\"\n",
    "        \n",
    "        from lib.customCallbacks import lossGradientLR,LRTensorBoard,eStop\n",
    "        from lib.customCallbacks import layerUnFreeze,trainableReport,timeLog\n",
    "        \n",
    "        import os\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras import optimizers\n",
    "        \n",
    "        # Random consistency seed\n",
    "        tf.random.set_seed(100)\n",
    "        \n",
    "        # losses\n",
    "        losses = {'names':tf.keras.losses.CategoricalCrossentropy(name='catergoricalCrossEntropy'),\n",
    "                  'boxes':tf.keras.losses.MeanSquaredError(name='MeanSquaredError')}\n",
    "        \n",
    "        loss_weights = {'names':1,'boxes':1}\n",
    "        loss_weigths = gridPoint.get('lossWeights',loss_weights)\n",
    "\n",
    "        # metrics\n",
    "        with tf.device(GPU):\n",
    "            metrics = {'names':[tf.keras.metrics.CategoricalAccuracy(name='CategoricalAccuracy'),\n",
    "                                tf.keras.metrics.Precision(name='Precision'),\n",
    "                                tf.keras.metrics.Recall(name='Recall')],\n",
    "                       'boxes':IoU}\n",
    "        \n",
    "        # compile model\n",
    "        self.model.compile(optimizer=optim,loss=losses,metrics=metrics,loss_weights=loss_weights)\n",
    "        # learning rate to be updated before fitting\n",
    "        \n",
    "        # create tensorboard logs & callbacks\n",
    "        logdir = os.path.join(\".\",\"logs\",self.modelName)\n",
    "        try:\n",
    "            os.rmdir(logir)\n",
    "        except:\n",
    "            pass\n",
    "        with tf.device(GPU):\n",
    "            tensorboard_cb = tf.keras.callbacks.TensorBoard(logdir,histogram_freq=1,embeddings_freq=1)\n",
    "        \n",
    "        # initiate learning rate reporter\n",
    "        lReport = LRTensorBoard(logdir)\n",
    "        \n",
    "        # learnin rate decay scheduler\n",
    "        lrParam = dict(initial_learning_rate=0.555)\n",
    "        lrParam.update(**lrScheduleParams)\n",
    "        lr_schedule = lossGradientLR(**lrParam)\n",
    "        # lr_schedule.lr to be updated before fitting\n",
    "        \n",
    "        # save best model weights\n",
    "        sFile = os.path.join(\".\",\"logs\",self.modelName,self.modelName+'.h5')\n",
    "        with tf.device(GPU):\n",
    "            saveBest = tf.keras.callbacks.ModelCheckpoint(filepath=sFile,monitor='val_loss',\n",
    "                                                          mode='min',save_best_only=True)\n",
    "        \n",
    "        # early stop callback\n",
    "        eBrake = eStop(**eStopParams)\n",
    "        \n",
    "        # layer trainability callback\n",
    "        trainabilityScheduler = layerUnFreeze(**trainabilityParams)\n",
    "        \n",
    "        # model trainability status reporter\n",
    "        trainabilityReporter = trainableReport(logdir)\n",
    "        \n",
    "        # epoch timer call back\n",
    "        timerCBack = timeLog(logdir)\n",
    "\n",
    "        # callback collection based on cbOptions\n",
    "        cbs = [tensorboard_cb,lReport,trainabilityReporter,timerCBack]\n",
    "        cbs.append(lr_schedule) if lrScheduleParams!={} else None\n",
    "        cbs.append(eBrake) if eStopParams!={} else None\n",
    "        cbs.append(trainabilityScheduler) if trainabilityParams!={} else None\n",
    "        cbs.append(saveBest) if sBest else None\n",
    "\n",
    "        # update fitParams\n",
    "        self.fParams = dict(callbacks=cbs)\n",
    "        \n",
    "        # record & update model Log\n",
    "        self.mLog.update(dict(optimizer=optim._name))\n",
    "        self.mLog.update(**lrScheduleParams)\n",
    "        self.mLog.update(**eStopParams)\n",
    "        self.mLog.update(**trainabilityParams)\n",
    "        self.mLog.update(**gridPoint)\n",
    "        \n",
    "        # update comilation status\n",
    "        self._compiled = True\n",
    "        \n",
    "    def fit(self,gridPoint,fitParams):\n",
    "        \"\"\" learner method\n",
    "        arguments:\n",
    "        ----------\n",
    "        gridPoint : dict of hyperparameters of the model, MUST include learning_rate\n",
    "        fitParams : dict of arguments to model.fit(), except dataset selection\n",
    "        \"\"\"\n",
    "        \n",
    "        assert self._compiled, \"attempt to fit uncompiled model\"\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras import backend as K\n",
    "\n",
    "        from lib.customCallbacks import lossGradientLR\n",
    "        \n",
    "        # Random consistency seed\n",
    "        tf.random.set_seed(100)\n",
    "        \n",
    "        # update optimiser learning rate\n",
    "        lr = gridPoint.get('learning_rate',0.555)\n",
    "        K.set_value(self.model.optimizer.learning_rate, lr)\n",
    "        \n",
    "        # update initial_epoch\n",
    "        init_epoch = fitParams.get('initial_epoch',0)\n",
    "        self.trainset.epoch = init_epoch\n",
    "        self.validationset.epoch = init_epoch\n",
    "        self.testset.epoch = init_epoch\n",
    "        \n",
    "        # update learning rate scheduler initial value\n",
    "        cbs = self.fParams.get('callbacks')\n",
    "        for cb in cbs:\n",
    "            if isinstance(cb,lossGradientLR):  # if scheduler was opted\n",
    "                cb.lr = [lr]\n",
    "                cb.lastAdjust = 0\n",
    "        \n",
    "        # update fitParams\n",
    "        self.fParams.update(**fitParams)\n",
    "        \n",
    "        # model fitting\n",
    "        with tf.device(GPU):\n",
    "            self.logger = self.model.fit(self.trainset,validation_data=self.validationset,**self.fParams)\n",
    "            \n",
    "        # record & update model Log\n",
    "        self.mLog.update(**gridPoint)\n",
    "        self.mLog.update(**fitParams)\n",
    "        self.mLog.update(logs=self.logger.history)\n",
    "            \n",
    "        self._fitted=True                 \n",
    "        \n",
    "    def visualiseFit(self):\n",
    "        \"\"\"display the model training history metrics\"\"\"\n",
    "        \n",
    "        assert self._fitted, \"unfit model\"\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        sns.set_theme()\n",
    "        \n",
    "        df = pd.DataFrame(self.logger.history)\n",
    "        \n",
    "        plt.figure(figsize=(16,8))        \n",
    "\n",
    "        plt.subplot(3,4,1)\n",
    "        ax = sns.lineplot(data=df,y='learning_rate',x=df.index)\n",
    "        ax.set_yscale('log')\n",
    "        plt.title('learning_rate')\n",
    "        ax.legend(['scheduled'])\n",
    "        ax.set(xlabel=None,ylabel=None);\n",
    "        \n",
    "        plt.subplot(3,4,5)\n",
    "        ax = sns.lineplot(data=df,y='lapTime',x=df.index)\n",
    "        plt.title('resource consumption')\n",
    "        ax.legend(['lapTime(s)'])\n",
    "        ax.set(xlabel=None,ylabel=None);\n",
    "        \n",
    "        plt.subplot(3,4,9)\n",
    "        total = len(self.model.layers)\n",
    "        ax = sns.lineplot(data=df,y='Trainable',x=df.index) # trainable layer count\n",
    "        ax = sns.lineplot(y=[total]*df.shape[0],x=df.index) # full layer count\n",
    "        ax.legend(['trainable','total'])\n",
    "        plt.title('Count of Trainable layers')\n",
    "        ax.set(xlabel='epoch',ylabel=None);\n",
    "\n",
    "        plt.subplot(2,4,2)\n",
    "        ax = sns.lineplot(data=df,y='boxes_IoU',x=df.index)\n",
    "        ax = sns.lineplot(data=df,y='val_boxes_IoU',x=df.index)\n",
    "        ax.legend(['training','validation'])\n",
    "        plt.title('Bounding Box IOU')\n",
    "        ax.set(xlabel=None,ylabel=None);\n",
    "\n",
    "        plt.subplot(2,4,6)\n",
    "        ax = sns.lineplot(data=df,y='boxes_loss',x=df.index)\n",
    "        ax = sns.lineplot(data=df,y='val_boxes_loss',x=df.index)\n",
    "        ax.legend(['training','validation'])\n",
    "        plt.title('Bounding Box loss')\n",
    "        ax.set(xlabel='epoch',ylabel=None);\n",
    "\n",
    "        plt.subplot(2,4,3)\n",
    "        ax = sns.lineplot(data=df,y='names_CategoricalAccuracy',x=df.index)\n",
    "        ax = sns.lineplot(data=df,y='val_names_CategoricalAccuracy',x=df.index)\n",
    "        ax.legend(['training','validation'])\n",
    "        plt.title('categorical accuracy')\n",
    "        ax.set(xlabel=None,ylabel=None);\n",
    "\n",
    "        plt.subplot(2,4,7)\n",
    "        ax = sns.lineplot(data=df,y='names_loss',x=df.index)\n",
    "        ax = sns.lineplot(data=df,y='val_names_loss',x=df.index)\n",
    "        ax.legend(['training','validation'])\n",
    "        plt.title('categorical crossentropy')\n",
    "        ax.set(xlabel='epoch',ylabel=None);\n",
    "\n",
    "        plt.subplot(2,4,4)\n",
    "        ax = sns.lineplot(data=df,y='names_Precision',x=df.index)\n",
    "        ax = sns.lineplot(data=df,y='val_names_Precision',x=df.index)\n",
    "        ax.legend(['training','validation'])\n",
    "        plt.title('average precision')\n",
    "        ax.set(xlabel=None,ylabel=None);\n",
    "\n",
    "        plt.subplot(2,4,8)\n",
    "        ax = sns.lineplot(data=df,y='names_Recall',x=df.index)\n",
    "        ax = sns.lineplot(data=df,y='val_names_Recall',x=df.index)\n",
    "        ax.legend(['training','validation'])\n",
    "        plt.title('average recall')\n",
    "        ax.set(xlabel='epoch',ylabel=None);\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \"\"\"evaluate on train & test data\"\"\"\n",
    "        \n",
    "        assert self._fitted, \"unfit model\"\n",
    "        \"\"\"\n",
    "        print(\"\\n----------\")\n",
    "        print(\"EVALUATION\")\n",
    "        print(\"----------\\n\")\"\"\"\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # evaluate on training data\n",
    "        with tf.device(GPU):\n",
    "            scores = self.model.evaluate(self.evalTrain,verbose=0)\n",
    "        evalCols = ['loss', 'names_loss', 'boxes_loss', 'names_CategoricalAccuracy',\n",
    "                    'names_Precision', 'names_Recall', 'boxes_IoU']\n",
    "        self.mLog.update(dict(zip(evalCols,scores)))\n",
    "        # evaluate on testing data\n",
    "        with tf.device(GPU):\n",
    "            scores = self.model.evaluate(self.evalTest,verbose=0)\n",
    "        evalCols = ['val_'+ec for ec in evalCols]\n",
    "        self.mLog.update(dict(zip(evalCols,scores)))\n",
    "        \n",
    "    def report(self):\n",
    "        \"\"\"report model performance on train & test data\"\"\"\n",
    "        \n",
    "        assert self._fitted, \"unfit model\"\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        from sklearn import metrics\n",
    "        pd.set_option('display.max_columns', 50)\n",
    "        \n",
    "        # display training history\n",
    "        self.visualiseFit() \n",
    "        \n",
    "        # display Logs\n",
    "        dLog = pd.DataFrame(data=np.array(list(self.mLog.values()),\n",
    "                                          dtype=object).reshape(1,-1),\n",
    "                            columns=list(self.mLog.keys()),dtype=object)\n",
    "        display(dLog)\n",
    "        \n",
    "        \"\"\"print(\"\\n----------\")\n",
    "        print(\"PREDICTION\")\n",
    "        print(\"----------\\n\")\"\"\"\n",
    "        \n",
    "        # predict on training data       \n",
    "        with tf.device(GPU):\n",
    "            [predTrainClass,predTrainBox] = self.model.predict(self.evalTrain,verbose=0)\n",
    "        tr_len = predTrainBox.shape[0]\n",
    "        [trainClass,trainBox] = [self.evalTrain.cName[:tr_len],self.evalTrain.bBox[:tr_len]]\n",
    "        predTrainBox = self.evalTrain.unSize(predTrainBox,np.arange(tr_len))\n",
    "        trainImages = self.evalTrain.imPath[:tr_len]\n",
    "        \n",
    "        # predict on testing data\n",
    "        with tf.device(GPU):\n",
    "            [predTestClass,predTestBox] = self.model.predict(self.evalTest,verbose=0)\n",
    "        tt_len = predTestBox.shape[0]\n",
    "        [testClass,testBox] = [self.evalTest.cName[:tt_len],self.evalTest.bBox[:tt_len]]\n",
    "        predTestBox = self.evalTest.unSize(predTestBox,np.arange(tt_len))\n",
    "        testImages = self.evalTest.imPath[:tt_len]\n",
    "        \n",
    "        # obtain confusion matrix\n",
    "        confTrain = metrics.multilabel_confusion_matrix(trainClass,(predTrainClass>=0.5)*1)\n",
    "        confTest = metrics.multilabel_confusion_matrix(testClass,(predTestClass>=0.5)*1)\n",
    "        \n",
    "        enc = self.trainset.enc\n",
    "        # visualise confusion metrics\n",
    "        confPlot1(enc.classes_,confTrain,confTest)\n",
    "        confPlot2(enc.classes_,confTrain,confTest) # testing 2 types of visualisation\n",
    "        \n",
    "        # visualise few images with predictions on training set\n",
    "        print(\"samples from TRAINING SET\")\n",
    "        sampleResult(trainImages,trainBox,predTrainBox,\n",
    "                     enc.inverse_transform(trainClass),enc.inverse_transform(predTrainClass))\n",
    "        print(\"samples from TESTING SET\")\n",
    "        sampleResult(testImages,testBox,predTestBox,\n",
    "                     enc.inverse_transform(testClass),enc.inverse_transform(predTestClass))\n",
    "        \n",
    "        \"\"\"self.resLog = mReport(self.modelName,\n",
    "                              self.evalTrain.imPath[:tr_len],[trainClass,predTrainClass,trainBox,predTrainBox],\n",
    "                              self.evalTest.imPath[:tt_len],[testClass,predTestClass,testBox,predTestBox],\n",
    "                              self.trainset.enc,verbose=1)\"\"\"\n",
    "\n",
    "    def htuneSave(self):\n",
    "        \"\"\"saving only the logs for hypertuning purpose\"\"\"\n",
    "        assert self._fitted, \"unfit model\"\n",
    "        import pickle\n",
    "        import os\n",
    "        path = os.path.join('.',\"deployables\")\n",
    "        fname = os.path.join(path,\"%s_attrib.gl\"%self.modelName) # file name\n",
    "        attribNames = ['labelEncoder','modelFunc','inputShape','evalResults']\n",
    "        attributes = [self.trainset.enc,self.mFunc,self.model.input_shape,self.mLog]\n",
    "        with open(fname, 'wb') as fh:\n",
    "            pickle.dump(dict(zip(attribNames,attributes)), fh) # pickling\n",
    "        \n",
    "    def save(self):\n",
    "        \"\"\"pickle each critical components to create a deployable model\"\"\"\n",
    "        \n",
    "        assert self._fitted, \"unfit model\"\n",
    "        \n",
    "        import pickle\n",
    "        import os\n",
    "        \n",
    "        path = os.path.join('.',\"deployables\")\n",
    "        try:\n",
    "            os.listdir(path) # check availability\n",
    "        except:\n",
    "            os.mkdir(path) # create folder\n",
    "        path = os.path.join(path,self.modelName)\n",
    "        try:\n",
    "            os.rmdir(path) # delete duplicate folder\n",
    "            os.mkdir(path) # create fresh folder\n",
    "        except:\n",
    "            try:\n",
    "                os.mkdir(path) # create fresh folder\n",
    "            except:\n",
    "                pass \n",
    "        \n",
    "        fname = os.path.join(path,\"%s_attrib.gl\"%self.modelName) # file name\n",
    "        attribNames = ['labelEncoder','modelFunc','inputShape','evalResults']\n",
    "        attributes = [self.trainset.enc,self.mFunc,self.model.input_shape,self.mLog]\n",
    "        with open(fname, 'wb') as fh:\n",
    "            pickle.dump(dict(zip(attribNames,attributes)), fh) # pickling\n",
    "        fname = os.path.join(path,\"%s_model.h5\"%self.modelName) \n",
    "        self.model.save(fname) # h5 model save\n",
    "        fname = os.path.join(path,\"%s_weights.h5\"%self.modelName) \n",
    "        self.model.save_weights(fname) # model weights\n",
    "        \n",
    "        tfpath = os.path.join(path,self.modelName,\"tfsave\")\n",
    "        try:\n",
    "            os.mkdir(path) # create tf save folder\n",
    "        except:\n",
    "            pass\n",
    "        self.model.save(tfpath) # tf save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a77402aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./lib/networkPipe.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a './lib/networkPipe.py'\n",
    "# -------------------------------------------------------------------------------------------------------------------------------\n",
    "# evaluation & logging\n",
    "lCols = ['modelName','loss','names_loss','boxes_loss','names_accuracy','names_precision','names_recall',\n",
    "         'boxes_IoU','val_loss','val_names_loss','val_boxes_loss','val_names_accuracy','val_names_precision',\n",
    "         'val_names_recall','val_boxes_IoU']\n",
    "\n",
    "def mReport(mName,trainImages,outSetA,testImages,outSetB,enc,verbose=1):\n",
    "    \"\"\"arguments:\n",
    "    ----------\n",
    "    mName : model names as string\n",
    "    trainImages : list of training image paths\n",
    "    outSetA : outputs for training dataset [trueY,predY,trueYbox,predYbox]\n",
    "    testImages : list of testing image paths\n",
    "    outSetB : outputs for testing dataset [testY,predTestY,testYbox,predTestYbox]\n",
    "    enc: fitted LabelBinarizer() object\n",
    "    \n",
    "    note: all box coordinates to be rescaled to orignal aspect ratios & sizes\n",
    "    \n",
    "    returns: None\n",
    "    \n",
    "    functionality:\n",
    "    --------------\n",
    "    updates resLog\n",
    "    displays classification report & confusion stats\n",
    "    displays sample results for training & testing datasets\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    trueY,predY,trueYbox,predYbox = outSetA\n",
    "    testY,predTestY,testYbox,predTestYbox = outSetB\n",
    "    \n",
    "    mLog = np.array([mName],dtype=object)\n",
    "    mLog = evalLog(mLog,*outSetA)\n",
    "    mLog = evalLog(mLog,*outSetB)\n",
    "\n",
    "    # append to results log\n",
    "    resLog = pd.DataFrame(mLog.reshape(1,-1),columns=lCols)\n",
    "    display(resLog)\n",
    "    resLog = dict(zip(lCols,mLog))\n",
    "    \n",
    "    if not verbose:\n",
    "        return resLog\n",
    "    \n",
    "    # obtain confusion matrix\n",
    "    confTrain = metrics.multilabel_confusion_matrix(trueY,(predY>=0.5)*1)\n",
    "    confTest = metrics.multilabel_confusion_matrix(testY,(predTestY>=0.5)*1)\n",
    "    \n",
    "    # visualise confusion metrics\n",
    "    confPlot1(enc.classes_,confTrain,confTest)\n",
    "    confPlot2(enc.classes_,confTrain,confTest) # testing 2 types of visualisation\n",
    "    \n",
    "    # visualise few images with predictions on training set\n",
    "    print(\"samples from TRAINING SET\")\n",
    "    sampleResult(trainImages,trueYbox,predYbox,enc.inverse_transform(trueY),enc.inverse_transform(predY))\n",
    "    print(\"samples from TESTING SET\")\n",
    "    sampleResult(testImages,testYbox,predTestYbox,enc.inverse_transform(testY),enc.inverse_transform(predTestY))\n",
    "    \n",
    "    return resLog\n",
    "\n",
    "def interBox(y_true,y_pred):\n",
    "    \"\"\"arguments:\n",
    "    ----------\n",
    "    y_true : true bounding box coordinates as list\n",
    "    y_pred : predicted bounding box coordinates as list\n",
    "    expected shape of arguments: (None,4)\n",
    "    \n",
    "    returns: coordinates of intersection of true & predicted bounding boxes (None,4)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    # intersection coordinates (x1,y1)\n",
    "    x1 = np.maximum(y_true[:,0],y_pred[:,0])\n",
    "    y1 = np.maximum(y_true[:,1],y_pred[:,1])\n",
    "    # intersection coordinates (x2,y2)\n",
    "    x2 = np.minimum(y_true[:,2],y_pred[:,2])\n",
    "    y2 = np.minimum(y_true[:,3],y_pred[:,3]) \n",
    "    \n",
    "    return np.array((x1,y1,x2,y2)).transpose()\n",
    "\n",
    "def evalLog(mLog,trueY,predY,trueBox,predBox):    \n",
    "    \"\"\"arguments:\n",
    "    ----------\n",
    "    mLog : array of current model metrics\n",
    "    trueClass : binarized true labels (None,196)\n",
    "    predClass : predicted logits (None,196)\n",
    "    trueBox : true bounding box coordinates (None,4)\n",
    "    predBox : predicted bounding box coordinates (None,4)\n",
    "    \n",
    "    returns : updated array of current model metrics\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    \n",
    "    with tf.device(GPU):\n",
    "        \n",
    "        # log of metrics & losses for training dataset\n",
    "        names_loss = tf.keras.losses.CategoricalCrossentropy()(trueY,predY).numpy()\n",
    "        boxes_loss = tf.keras.losses.MeanSquaredError()(trueBox,predBox).numpy()\n",
    "        loss_sum = names_loss+boxes_loss\n",
    "\n",
    "        mLog = np.append(mLog,loss_sum)\n",
    "        mLog = np.append(mLog,names_loss)\n",
    "        mLog = np.append(mLog,boxes_loss)\n",
    "        mLog = np.append(mLog,tf.keras.metrics.CategoricalAccuracy()(trueY,predY).numpy())\n",
    "        mLog = np.append(mLog,tf.keras.metrics.Precision()(trueY,predY).numpy())\n",
    "        mLog = np.append(mLog,tf.keras.metrics.Recall()(trueY,predY).numpy())\n",
    "        mLog = np.append(mLog,IoU(trueBox.astype('float32'),predBox.astype('float32')).numpy().mean())\n",
    "    \n",
    "    return mLog\n",
    "\n",
    "def confPlot1(labels,confTrain,confTest):\n",
    "    \"\"\"arguments:\n",
    "    ----------\n",
    "    labels : list of classes\n",
    "    confTrain : multi-label confusion matrix for prediction on training set\n",
    "    confTest : multi-label confusion matrix for prediction on test set\n",
    "    \n",
    "    functionality:\n",
    "    --------------\n",
    "    displays subplots pf confusion metrics\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np    \n",
    "    sns.set_theme()\n",
    "    \n",
    "    xls = np.arange(len(labels))\n",
    "    plt.figure(figsize=(16,5))\n",
    "    plt.suptitle('TRAINING DATASET                                                                                               TESTING DATASET')\n",
    "    \n",
    "    p1 = plt.subplot(2,4,1)\n",
    "    plt.fill_between(x=xls,y1=confTrain[:,0,0],color='#FFA15A')\n",
    "    p1.xaxis.set_ticks([])\n",
    "    plt.title(\"TRUE NEGATIVES\")\n",
    "    \n",
    "    p2 = plt.subplot(2,4,2)#,sharey=p1)\n",
    "    plt.fill_between(x=xls,y1=confTrain[:,0,1],color='#19D3F3')\n",
    "    p2.xaxis.set_ticks([])\n",
    "    plt.title(\"FALSE POSITIVES\")\n",
    "    \n",
    "    p5 = plt.subplot(2,4,5)\n",
    "    plt.fill_between(x=xls,y1=confTrain[:,1,0],color='#B82E2E')\n",
    "    p5.xaxis.set_ticks([])\n",
    "    plt.title(\"FALSE NEGATIVES\")\n",
    "    \n",
    "    p6 = plt.subplot(2,4,6)#,sharey=p5)\n",
    "    plt.fill_between(x=xls,y1=confTrain[:,1,1],color='#109618')\n",
    "    p6.xaxis.set_ticks([])\n",
    "    plt.title(\"TRUE POSITIVES\")\n",
    "    \n",
    "    p3 = plt.subplot(2,4,3)#,sharey=p1)\n",
    "    plt.fill_between(x=xls,y1=confTest[:,0,0],color='#FFA15A')\n",
    "    p3.xaxis.set_ticks([])\n",
    "    plt.title(\"TRUE NEGATIVES\")\n",
    "    \n",
    "    p4 = plt.subplot(2,4,4)#,sharey=p1)\n",
    "    plt.fill_between(x=xls,y1=confTest[:,0,1],color='#19D3F3')\n",
    "    p4.xaxis.set_ticks([])\n",
    "    plt.title(\"FALSE POSITIVES\")\n",
    "    \n",
    "    p7 = plt.subplot(2,4,7)#,sharey=p5)\n",
    "    plt.fill_between(x=xls,y1=confTest[:,1,0],color='#B82E2E')\n",
    "    p7.xaxis.set_ticks([])\n",
    "    plt.title(\"FALSE NEGATIVES\")\n",
    "    \n",
    "    p8 = plt.subplot(2,4,8)#,sharey=p5)\n",
    "    plt.fill_between(x=xls,y1=confTest[:,1,1],color='#109618')\n",
    "    p8.xaxis.set_ticks([])\n",
    "    plt.title(\"TRUE POSITIVES\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def confPlot2(labels,confTrain,confTest):\n",
    "    \"\"\"arguments:\n",
    "    ----------\n",
    "    labels : list of classes\n",
    "    confTrain : multi-label confusion matrix for prediction on training set\n",
    "    confTest : multi-label confusion matrix for prediction on test set\n",
    "    \n",
    "    functionality:\n",
    "    --------------\n",
    "    displays overlayed confusion metrics in log scale\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np    \n",
    "    sns.set_theme()\n",
    "    \n",
    "    plt.figure(figsize=(16,5))\n",
    "    xls = np.arange(len(labels))\n",
    "    \n",
    "    p1=plt.subplot(1,2,1)\n",
    "    plt.title(\"TRAINING DATASET\")\n",
    "    plt.fill_between(x=xls,y1=confTrain[:,0,0],color='#FFA15A')\n",
    "    plt.fill_between(x=xls,y1=confTrain[:,0,1],color='#19D3F3')\n",
    "    plt.fill_between(x=xls,y1=confTrain[:,1,0],color='#B82E2E')\n",
    "    plt.fill_between(x=xls,y1=confTrain[:,1,1],color='#109618')\n",
    "    p1.xaxis.set_ticks([])\n",
    "    plt.yscale(\"log\") \n",
    "    \n",
    "    p2=plt.subplot(1,2,2,sharey=p1)\n",
    "    plt.title(\"TESTING DATASET\")\n",
    "    plt.fill_between(x=xls,y1=confTest[:,0,0],color='#FFA15A')\n",
    "    plt.fill_between(x=xls,y1=confTest[:,0,1],color='#19D3F3')\n",
    "    plt.fill_between(x=xls,y1=confTest[:,1,0],color='#B82E2E')\n",
    "    plt.fill_between(x=xls,y1=confTest[:,1,1],color='#109618')\n",
    "    p2.xaxis.set_ticks([])\n",
    "    plt.yscale(\"log\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def sampleResult(imPath,trueBox,predBox,trueClass,predClass):\n",
    "    \"\"\" display 3 sample images with true & predicted targets\n",
    "    arguments:\n",
    "    ----------\n",
    "    imPath : paths of images\n",
    "    trueBox : true coordinates of bounding box\n",
    "    predBox : predicted coordinates of bounding box\n",
    "    trueClass : list of true class names (strings)\n",
    "    predClass : list of predicted class names (strings)\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns  \n",
    "    import cv2\n",
    "    sns.set_theme()\n",
    "    \n",
    "    tkpi = tf.keras.preprocessing.image\n",
    "    indices = np.random.choice(range(len(imPath)),3,replace=False) # displays 3 sample images\n",
    "    plt.figure(figsize=(16,5))\n",
    "    for i,index in enumerate(indices):\n",
    "        ax = plt.subplot(1,len(indices),i+1)\n",
    "        img = tkpi.img_to_array(tkpi.load_img(imPath[index]))\n",
    "        tBox = trueBox[index]    \n",
    "        pBox = predBox[index]\n",
    "        iBox = interBox(tBox.reshape(-1,4),pBox.reshape(-1,4))[0]\n",
    "        cv2.rectangle(img,tBox[:2],tBox[2:],(0,255,0),2)\n",
    "        cv2.rectangle(img,pBox[:2],pBox[2:],(255,0,255),2)\n",
    "        cv2.rectangle(img,iBox[:2],iBox[2:],(0,0,255),4)\n",
    "        img = tkpi.array_to_img(img)\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"True Class: %s\"%trueClass[index],fontdict=dict(fontsize=10))\n",
    "        plt.xlabel(\"Prediction: %s\"%predClass[index])\n",
    "        plt.ylabel(\"IOU: %.4f\"%IOU(tBox.reshape(-1,4)*1.0,pBox.reshape(-1,4)*1.0))\n",
    "        ax.axes.get_xaxis().set_ticks([])\n",
    "        ax.axes.get_yaxis().set_ticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f2d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
